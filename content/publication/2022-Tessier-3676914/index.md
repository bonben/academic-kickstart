+++
title = "Rethinking Weight Decay for Efficient Neural Network Pruning"
date = 2022-03-01T00:00:00Z
authors = ['Hugo Tessier', 'Vincent Gripon', 'Mathieu Leonardon', 'Matthieu Arzel', 'Thomas Hannagan', 'David Bertrand']
publication_types = ["2"]
abstract = "Introduced in the late 1980s for generalization purposes, pruning has now become a staple for compressing deep neural networks. Despite many innovations in recent decades, pruning approaches still face core issues that hinder their performance or scalability. Drawing inspiration from early work in the field, and especially the use of weight decay to achieve sparsity, we introduce Selective Weight Decay (SWD), which carries out efficient, continuous pruning throughout training. Our approach, theoretically grounded on Lagrangian smoothing, is versatile and can be applied to multiple tasks, networks, and pruning structures. We show that SWD compares favorably to state-of-the-art approaches, in terms of performance-to-parameters ratio, on the CIFAR-10, Cora, and ImageNet ILSVRC2012 datasets."
featured = true
publication = "Journal of Imaging (MDPI)"
doi = "10.3390/jimaging8030064"
url_pdf = "https://hal.archives-ouvertes.fr/hal-03675138/document"
+++
